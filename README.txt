================================================================================
                 HACK-EARTH: ENERGY EFFICIENCY MODEL LAB
                            Interactive ML Platform
                              Version 1.0 (2025)
================================================================================

PROJECT OVERVIEW
================================================================================

Hack-Earth is an interactive machine learning laboratory built with Streamlit 
that enables comprehensive analysis, modeling, and prediction of building energy 
efficiency. The platform uses advanced machine learning techniques to model the 
relationship between building physical characteristics and their heating/cooling 
energy requirements (Based on the pre loaded dataset!).

The application is designed for researchers, engineers, Analysts and sustainability 
professionals to:
  â€¢ Load and analyze energy efficiency datasets
  â€¢ Train and compare multiple ML models for regression and classification tasks
  â€¢ Track computational emissions using CodeCarbon
  â€¢ Evaluate model performance with detailed diagnostics
  â€¢ Perform "what-if" scenario analysis for predictive testing

PROJECT STRUCTURE
================================================================================

Root Directory: /workspaces/Hack-Earth

Files:
  â€¢ app.py                    - Main Streamlit application (516 lines)
  â€¢ emissions.csv             - Sample CodeCarbon emissions tracking data
  â€¢ emissions.csv*.bak        - Backup files of emissions data
  â€¢ LICENSE                   - Apache License 2.0
  â€¢ README.txt                - This file

KEY FEATURES & FUNCTIONALITY
================================================================================

1. DATA MANAGEMENT
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Dataset Sources:
   â€¢ Upload CSV: Import your own energy efficiency dataset
   â€¢ Generate Synthetic: Pre-loaded 500,000 realistic building records
     (UCI Energy Efficiency Dataset-inspired schema)
   
   Supported Dataset Schema:
     - RelativeCompactness: Building shape compactness ratio (0.5-1.0)
     - SurfaceArea: Total building surface area in mÂ² (50-500)
     - WallArea: Total wall area in mÂ² (20-300)
     - RoofArea: Total roof area in mÂ² (30-200)
     - OverallHeight: Building height in meters (2.5-10)
     - Orientation: Cardinal direction (North, South, East, West)
     - GlazingAreaDistribution: Window distribution pattern
       Options: Uniform, North-heavy, South-heavy, East-heavy, West-heavy
     - BuildingType: Category of building
       Options: Residential, Commercial, Industrial
     - HeatingLoad: Target variable for regression (kWh)
     - CoolingLoad: Alternative target variable (kWh)
   
   Data Preprocessing:
   â€¢ Automatic detection and removal of NaN/Inf values
   â€¢ Duplicate row elimination
   â€¢ Categorical encoding using one-hot encoding (drop_first=True)
   â€¢ Optional feature standardization (StandardScaler)
   â€¢ Automatic data subsetting for performance optimization


2. TASK MODES
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   A. REGRESSION MODE
      Predicts continuous heating/cooling load values
      
      Supported Regressors:
      â€¢ LinearRegression - Baseline linear model
      â€¢ Ridge - L2-regularized linear regression (alpha=1.0 default)
      â€¢ Lasso - L1-regularized linear regression (alpha=1.0 default)
      â€¢ RandomForestRegressor - 200 trees ensemble (n_jobs=-1)
      â€¢ GradientBoostingRegressor - Sequential tree ensemble
      â€¢ XGBRegressor - Gradient boosting with advanced features
        (400 estimators, 0.05 learning rate, hist tree_method)
      â€¢ LGBMRegressor - Light gradient boosting (if installed)
      â€¢ CatBoostRegressor - Categorical gradient boosting (if installed)
      
      Evaluation Metrics:
      â€¢ RÂ² Score: Coefficient of determination (0-1, higher is better)
      â€¢ MSE: Mean Squared Error (lower is better)
      â€¢ RMSE: Root Mean Squared Error (lower is better)
      â€¢ Cross-Validation Score (optional 5-fold CV)
   
   B. CLASSIFICATION MODE
      Bins continuous values into discrete classes for classification
      
      Supported Classifiers:
      â€¢ LogisticRegression - Linear probabilistic classifier
      â€¢ RandomForestClassifier - 300 tree ensemble (n_jobs=-1)
      â€¢ GradientBoostingClassifier - Sequential tree classifier
      â€¢ XGBClassifier - Boosted tree classifier with mlogloss metric
      â€¢ LGBMClassifier - Light gradient boosting classifier
      â€¢ CatBoostClassifier - Categorical gradient boosting classifier
      
      Classification Configuration:
      â€¢ Target Binning: User selects base column (default: HeatingLoad)
      â€¢ Number of Classes: 3-6 bins configurable
      â€¢ Binning Strategy:
        - Quantile (pd.qcut): Creates balanced classes by percentiles
        - Uniform (pd.cut): Uses equal-width ranges
      
      Evaluation Metrics:
      â€¢ Accuracy: Proportion of correct predictions
      â€¢ F1 Score: Weighted harmonic mean of precision and recall
      â€¢ Precision: True positives / (true positives + false positives)
      â€¢ Recall: True positives / (true positives + false negatives)
      â€¢ Confusion Matrix: Visualization of classification performance
      â€¢ Cross-Validation Score (optional 5-fold CV)


3. MODEL TRAINING PIPELINE
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Process Flow:
   
   1. Data Loading & Validation
      â”œâ”€ Load CSV or generate 500k synthetic samples
      â”œâ”€ Remove NaN/Inf values
      â”œâ”€ Eliminate duplicates
      â””â”€ Validate dataset integrity
   
   2. Feature Engineering
      â”œâ”€ Select features (all columns except target)
      â”œâ”€ One-hot encode categorical variables
      â”œâ”€ Optionally subsample large datasets (default: 200k rows max)
      â””â”€ Build feature matrix X
   
   3. Target Preparation
      â”œâ”€ For Regression: Use raw continuous values
      â”œâ”€ For Classification:
      â”‚  â”œâ”€ Apply jitter to handle duplicate edges (quantile mode)
      â”‚  â””â”€ Bin values into n_bins classes
      â””â”€ Build target vector y
   
   4. Train-Test Split
      â”œâ”€ Stratified split for classification tasks
      â”œâ”€ Random split for regression tasks
      â”œâ”€ Default test_size: 20% (configurable)
      â”œâ”€ Controlled randomization via random_state seed
      â””â”€ Generates X_train, X_test, y_train, y_test
   
   5. Feature Scaling (Optional)
      â”œâ”€ StandardScaler applied to numeric features only
      â”œâ”€ Fit on training data
      â”œâ”€ Transform applied to test data
      â””â”€ Benefits: Linear models, LogisticRegression, SVM
   
   6. Model Training
      â”œâ”€ Initialize selected models with optimized hyperparameters
      â”œâ”€ Fit each model to training data
      â”œâ”€ Record training time (seconds)
      â””â”€ Optional: Track CO2 emissions during training (CodeCarbon)
   
   7. Model Inference
      â”œâ”€ Generate predictions on test set
      â”œâ”€ Record inference time (seconds)
      â””â”€ Compute performance metrics
   
   8. Optional Cross-Validation
      â”œâ”€ 5-fold stratified CV for classification
      â”œâ”€ 5-fold CV with RÂ² scoring for regression
      â”œâ”€ Compute mean CV score
      â””â”€ Compute std of CV scores (shown in tooltip)
   
   9. Results Aggregation
      â”œâ”€ Compile all metrics into results dictionary
      â”œâ”€ Sort by primary metric (RÂ² or Accuracy)
      â”œâ”€ Identify and highlight best-performing model
      â””â”€ Display comparative results table


4. EMISSIONS TRACKING & SUSTAINABILITY
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   CodeCarbon Integration:
   â€¢ Automatically measures COâ‚‚ emissions during model training
   â€¢ Tracks resource consumption (CPU, GPU, RAM power draw)
   â€¢ Records location-aware grid emissions data
   
   Metrics Tracked:
   â€¢ Duration: Training time in seconds
   â€¢ Emissions: COâ‚‚ equivalent in kilograms
   â€¢ Emissions Rate: kg COâ‚‚ per second
   â€¢ CPU/GPU/RAM Power: Power consumption in watts
   â€¢ Energy Consumed: Total energy in kWh
   â€¢ Location Data: Country, region, cloud provider (if applicable)
   â€¢ Hardware Info: CPU model, GPU count, RAM size, Python version
   
   Stored Data:
   â€¢ emissions.csv: Central log of all training runs
   â€¢ Columns: 42 including timestamp, project_name, run_id, experiment_id
   
   Example Entry (sample from emissions.csv):
   Timestamp: 2026-02-02T06:08:26
   Emissions: 2.24e-06 kg COâ‚‚
   Emissions Rate: 2.08e-06 kg COâ‚‚/sec
   Duration: 1.076 seconds
   CPU Power: 28.00 W
   GPU Power: 0.0 W
   RAM Power: 10.0 W


5. DIAGNOSTIC VISUALIZATIONS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Regression Diagnostics:
   â€¢ Residual Distribution Plot
     - Histogram of (y_true - y_pred)
     - Indicates bias and variance in predictions
     - Ideally centered near zero with normal distribution
   
   â€¢ True vs Predicted Scatter Plot
     - Scatter plot: y_true (x-axis) vs y_pred (y-axis)
     - Sample: 10,000 random test points for clarity
     - Ideal: Points cluster along y=x diagonal
     - Deviations indicate systematic prediction errors
   
   Classification Diagnostics:
   â€¢ Confusion Matrix Heatmap
     - True label (y-axis) vs Predicted label (x-axis)
     - Diagonal entries show correct predictions
     - Off-diagonal show misclassifications
     - Color intensity: Frequency of predictions
   
   â€¢ Class Distribution Bar Plot
     - Histogram of test set class labels
     - Shows class balance/imbalance
     - Important for interpreting classification metrics


6. SCENARIO TESTER: WHAT-IF ANALYSIS (Still under Developmental Phase!)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Interactive prediction interface for hypothetical building scenarios
   
   Input Parameters:
   â€¢ RelativeCompactness (slider)
     - Range: Data min-max with 5% padding
     - Default: 0.8
     - Impact: Higher values = more compact = better efficiency
   
   â€¢ SurfaceArea (slider)
     - Range: Data min-max with 5% padding
     - Default: 220.0 mÂ²
     - Impact: Larger area = higher heating/cooling demand
   
   â€¢ WallArea (slider)
     - Range: Data min-max with 5% padding
     - Default: 130.0 mÂ²
     - Impact: More wall area = more thermal loss potential
   
   â€¢ RoofArea (slider)
     - Range: Data min-max with 5% padding
     - Default: 95.0 mÂ²
     - Impact: Larger roof = more solar gain/loss
   
   â€¢ OverallHeight (slider)
     - Range: Data min-max with 5% padding
     - Default: 3.2 m
     - Impact: Taller buildings = different pressure zones
   
   â€¢ Orientation (dropdown)
     - Options: North, South, East, West
     - Impact: Affects solar heat gain patterns
   
   â€¢ GlazingAreaDistribution (dropdown)
     - Options: Uniform, North-heavy, South-heavy, East-heavy, West-heavy
     - Impact: Window orientation affects heating/cooling loads
   
   â€¢ BuildingType (dropdown)
     - Options: Residential, Commercial, Industrial
     - Impact: Different usage patterns and efficiency standards
   
   Processing Pipeline:
   1. Create single-row DataFrame with input values
   2. Apply same one-hot encoding as training data
   3. Align columns to match training feature set
   4. Apply StandardScaler (if used during training)
   5. Generate predictions from all trained models
   6. Display comparative predictions in table format


7. MODEL HYPERPARAMETERS
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Linear Models:
   â€¢ LinearRegression: No hyperparameters (fit_intercept=True default)
   â€¢ Ridge: alpha=1.0 (default)
   â€¢ Lasso: alpha=1.0 (default)
   â€¢ LogisticRegression: max_iter=2000
   
   Tree Ensemble Models:
   â€¢ RandomForestRegressor: n_estimators=200, n_jobs=-1, random_state=42
   â€¢ RandomForestClassifier: n_estimators=300, n_jobs=-1, random_state=42
   â€¢ GradientBoostingRegressor: random_state=42, other defaults
   â€¢ GradientBoostingClassifier: random_state=42
   
   Advanced Boosting Models:
   â€¢ XGBRegressor:
     - n_estimators=400
     - learning_rate=0.05
     - subsample=0.7 (70% sample ratio per iteration)
     - colsample_bytree=0.8 (80% feature ratio per iteration)
     - tree_method="hist" (Fast GPU-accelerated histogram)
     - random_state=42
   
   â€¢ XGBClassifier:
     - Same as XGBRegressor
     - eval_metric="mlogloss" (Multi-class loss)
   
   â€¢ LGBMRegressor/LGBMClassifier:
     - Uses LightGBM defaults
     - random_state=42
   
   â€¢ CatBoostRegressor/CatBoostClassifier:
     - verbose=0 (No logging during training)
     - random_state=42


TECHNICAL ARCHITECTURE
================================================================================

Framework & Libraries:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Application Stack                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Frontend:        Streamlit (Interactive web UI)                         â”‚
â”‚ Backend:         Python 3.12                                            â”‚
â”‚ Data Processing: pandas, NumPy                                          â”‚
â”‚ ML Algorithms:   scikit-learn, XGBoost, LightGBM, CatBoost             â”‚
â”‚ Evaluation:      sklearn.metrics                                        â”‚
â”‚ Visualization:   matplotlib.pyplot                                      â”‚
â”‚ Sustainability:  CodeCarbon (emissions tracking)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Required Dependencies:
  â€¢ numpy           - Numerical computing
  â€¢ pandas          - Data manipulation and analysis
  â€¢ scikit-learn    - Core ML algorithms and metrics
  â€¢ streamlit       - Web UI framework
  â€¢ matplotlib      - Plotting and visualization

Optional Dependencies:
  â€¢ xgboost         - Extreme Gradient Boosting (XGB models)
  â€¢ lightgbm        - Light Gradient Boosting (LGBM models)
  â€¢ catboost        - Categorical Boosting (CatBoost models)
  â€¢ codecarbon      - Carbon emissions tracking


ALGORITHM DETAILS
================================================================================

Regression Models Overview:

1. LINEAR REGRESSION
   â€¢ Equation: y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™
   â€¢ Uses: Ordinary Least Squares (OLS) optimization
   â€¢ Pros: Interpretable, fast, baseline model
   â€¢ Cons: Assumes linear relationship, sensitive to outliers

2. RIDGE REGRESSION (L2 Regularization)
   â€¢ Equation: min ||y - XÎ²||Â² + Î»||Î²||Â²
   â€¢ Lambda (Î±): Controls regularization strength
   â€¢ Pros: Handles multicollinearity, prevents overfitting
   â€¢ Cons: Shrinks all coefficients (doesn't zero out)

3. LASSO REGRESSION (L1 Regularization)
   â€¢ Equation: min ||y - XÎ²||Â² + Î»||Î²||
   â€¢ Lambda (Î±): Controls regularization strength
   â€¢ Pros: Feature selection (zeros out unimportant features)
   â€¢ Cons: Unstable with collinear features

4. RANDOM FOREST REGRESSOR
   â€¢ Ensemble: 200 decision trees (n_estimators=200)
   â€¢ Split Strategy: Greedy search for best split
   â€¢ Pros: Handles non-linearity, feature interactions, robust
   â€¢ Cons: Less interpretable, prone to overfitting without tuning

5. GRADIENT BOOSTING REGRESSOR
   â€¢ Sequential: Builds trees to correct previous errors
   â€¢ Loss: Squared error (default)
   â€¢ Pros: Excellent predictive power, handles interactions
   â€¢ Cons: Computationally intensive, hyperparameter-sensitive

6. XGBOOST REGRESSOR
   â€¢ Algorithm: Second-order gradient boosting with regularization
   â€¢ Tree Method: Histogram-based (fast approximation)
   â€¢ Regularization: L1/L2 penalties on leaf weights
   â€¢ Learning Rate: 0.05 (slower, more robust learning)
   â€¢ Subsample: 70% of rows per iteration (reduces variance)
   â€¢ Colsample: 80% of features per iteration
   â€¢ Pros: State-of-art performance, handles sparse data well
   â€¢ Cons: Complex to tune, slow predictions if many trees

7. LIGHTGBM REGRESSOR
   â€¢ Algorithm: Leaf-wise tree building (vs level-wise)
   â€¢ Split Criterion: Greedy feature selection
   â€¢ Pros: Faster training, memory efficient, handles large datasets
   â€¢ Cons: Prone to overfitting with small datasets

8. CATBOOST REGRESSOR
   â€¢ Algorithm: Gradient boosting with categorical features
   â€¢ Native Handling: Categorical variables without encoding
   â€¢ Ordered Boosting: Prevents overfitting (special mechanism)
   â€¢ Pros: Native categorical support, strong baseline, fast GPU
   â€¢ Cons: Less flexibility than XGBoost

Classification Models Overview:

1. LOGISTIC REGRESSION
   â€¢ Equation: P(y=1|x) = 1 / (1 + e^(-Î²Â·x))
   â€¢ Loss: Binary/Multi-class cross-entropy
   â€¢ Pros: Interpretable probabilities, fast, good baseline
   â€¢ Cons: Assumes linear decision boundaries

2. RANDOM FOREST CLASSIFIER
   â€¢ Ensemble: 300 decision trees (n_estimators=300)
   â€¢ Voting: Majority class vote from all trees
   â€¢ Pros: Handles non-linear boundaries, robust
   â€¢ Cons: Overfitting risk, black-box predictions

3. GRADIENT BOOSTING CLASSIFIER
   â€¢ Sequential: Builds trees to correct misclassified samples
   â€¢ Loss: Log loss (cross-entropy)
   â€¢ Pros: Strong performance, feature importance ranking
   â€¢ Cons: Sensitive to hyperparameters, slow training

4. XGBOOST CLASSIFIER
   â€¢ Multi-class Loss: mlogloss (multinomial cross-entropy)
   â€¢ Tree Method: Histogram-based approximation
   â€¢ Optimization: Second-order gradient descent (Newton's method)
   â€¢ Pros: Fast convergence, best-in-class performance
   â€¢ Cons: Complex tuning, extensive hyperparameter space

5. LIGHTGBM CLASSIFIER
   â€¢ Categorical Features: Native support without encoding
   â€¢ Leaf Growth: Leaf-wise (lower loss improvement)
   â€¢ Pros: Fastest training, memory efficient
   â€¢ Cons: Overfitting on small datasets, less stable

6. CATBOOST CLASSIFIER
   â€¢ Special Treatment: Categorical features optimized internally
   â€¢ Bootstrap: Bayesian bootstrap for regularization
   â€¢ Pros: Robust defaults, best for categorical data
   â€¢ Cons: Less flexible, slower predictions


DATA SYNTHETIC GENERATION ALGORITHM
================================================================================

Function: generate_synthetic_500k(seed=42, n_rows=500_000)

Purpose: Creates realistic building energy efficiency dataset mimicking UCI dataset

Process:

1. INITIALIZE RANDOM NUMBER GENERATOR
   â”œâ”€ Seed: 42 (reproducibility)
   â””â”€ Generator: NumPy 2.0+ default_rng (PCG64 algorithm)

2. GENERATE CONTINUOUS FEATURES
   
   RelativeCompactness (Uniform Distribution)
   â”œâ”€ Range: 0.5 to 1.0
   â”œâ”€ Simulates: Building shape efficiency (sphere=1.0)
   â””â”€ Rounding: 3 decimal places
   
   SurfaceArea (Uniform Distribution)
   â”œâ”€ Range: 50 to 500 mÂ²
   â”œâ”€ Simulates: Total external surface (affects heat loss)
   â””â”€ Rounding: 2 decimal places
   
   WallArea (Uniform Distribution)
   â”œâ”€ Range: 20 to 300 mÂ²
   â”œâ”€ Simulates: Vertical opaque surface
   â””â”€ Rounding: 2 decimal places
   
   RoofArea (Uniform Distribution)
   â”œâ”€ Range: 30 to 200 mÂ²
   â”œâ”€ Simulates: Horizontal surface (solar/sky radiation)
   â””â”€ Rounding: 2 decimal places
   
   OverallHeight (Uniform Distribution)
   â”œâ”€ Range: 2.5 to 10 m
   â”œâ”€ Simulates: Number of stories effect
   â””â”€ Rounding: 2 decimal places

3. GENERATE CATEGORICAL FEATURES
   
   Orientation (Categorical)
   â”œâ”€ Values: North, South, East, West
   â””â”€ Distribution: Uniform random selection
   
   GlazingAreaDistribution (Categorical)
   â”œâ”€ Values: Uniform, North-heavy, South-heavy, East-heavy, West-heavy
   â””â”€ Distribution: Uniform random selection
   
   BuildingType (Categorical)
   â”œâ”€ Values: Residential, Commercial, Industrial
   â””â”€ Distribution: Uniform random selection

4. GENERATE HEATING LOAD TARGET (kWh)
   
   Formula Components:
   base = 8 kWh
   
   Compactness Effect:
   â”œâ”€ Coefficient: 35
   â”œâ”€ Relationship: (1.05 - RelativeCompactness)
   â”œâ”€ Logic: Lower compactness = more surface = higher losses
   â””â”€ Impact Range: Â±17.5 kWh
   
   Surface Area Effect:
   â”œâ”€ Coefficient: 0.015
   â”œâ”€ Relationship: (SurfaceArea - 200)
   â””â”€ Impact Range: Â±4.5 kWh
   
   Wall Area Effect:
   â”œâ”€ Coefficient: 0.01
   â”œâ”€ Relationship: (WallArea - 120)
   â””â”€ Impact Range: Â±1.8 kWh
   
   Roof Area Effect:
   â”œâ”€ Coefficient: 0.02
   â”œâ”€ Relationship: (RoofArea - 90)
   â””â”€ Impact Range: Â±2.2 kWh
   
   Height Effect:
   â”œâ”€ Coefficient: 0.5
   â”œâ”€ Relationship: (OverallHeight - 3)
   â””â”€ Impact Range: Â±3.5 kWh
   
   Noise:
   â”œâ”€ Distribution: Normal(Î¼=0, Ïƒ=4)
   â”œâ”€ Represents: Unmeasured factors, measurement error
   â””â”€ Impact: Random variation up to Â±12 kWh (95% bounds)
   
   Final: Round to 2 decimals

5. GENERATE COOLING LOAD TARGET (kWh)
   
   Formula Components:
   base = 10 kWh (higher baseline due to HVAC efficiency)
   
   Compactness Effect:
   â”œâ”€ Coefficient: 28
   â”œâ”€ Relationship: (RelativeCompactness - 0.7)
   â”œâ”€ Logic: Compact buildings harder to cool (less mass for thermal storage)
   â””â”€ Impact Range: Â±8.4 kWh
   
   Surface Area Effect:
   â”œâ”€ Coefficient: 0.012
   â””â”€ Relationship: (SurfaceArea - 200)
   
   Wall Area Effect:
   â”œâ”€ Coefficient: 0.008
   â””â”€ Relationship: (WallArea - 120)
   
   Roof Area Effect:
   â”œâ”€ Coefficient: 0.015
   â””â”€ Relationship: (RoofArea - 90)
   
   Height Effect:
   â”œâ”€ Coefficient: 0.3
   â””â”€ Relationship: (OverallHeight - 3)
   
   Solar Gain Effect (South/West Orientation):
   â”œâ”€ South-heavy: +3.0 kWh (maximum solar gain)
   â”œâ”€ West-heavy: +1.5 kWh (afternoon sun)
   â””â”€ Other: 0 kWh (North-heavy, North, East, uniform)
   
   Noise:
   â”œâ”€ Distribution: Normal(Î¼=0, Ïƒ=4)
   â””â”€ Impact: Random variation
   
   Final: Round to 2 decimals


USER INTERFACE WALKTHROUGH
================================================================================

Sidebar Organization:

â”Œâ”€ Section 1: Data
â”‚  â”œâ”€ Choose source: Upload CSV / Generate synthetic (500k rows)
â”‚  â”œâ”€ Display stats: Row count, column count
â”‚  â””â”€ Preview option: Show first 10 rows of data
â”‚
â”œâ”€ Section 2: Task & Target
â”‚  â”œâ”€ Select task: Regression or Classification
â”‚  â”œâ”€ Regression target: Choose from numeric columns
â”‚  â””â”€ Classification:
â”‚     â”œâ”€ Select base column for binning
â”‚     â”œâ”€ Number of bins (3-6)
â”‚     â””â”€ Binning strategy: Quantile vs Uniform
â”‚
â”œâ”€ Section 3: Split & Sample
â”‚  â”œâ”€ Test size (%): 10-50 range (default 20%)
â”‚  â”œâ”€ Random seed: Control reproducibility (default 42)
â”‚  â””â”€ Max rows to subsample: Limit training data (default 200k)
â”‚
â”œâ”€ Section 4: Models & Options
â”‚  â”œâ”€ Select models: Multi-select from available models
â”‚  â”œâ”€ Scale features: Toggle StandardScaler (default True)
â”‚  â”œâ”€ Cross-validation: Toggle 5-fold CV (default False)
â”‚  â””â”€ Track emissions: Toggle CodeCarbon tracking (default True)
â”‚
â””â”€ Section 5: Run
   â””â”€ ğŸš€ Train & Compare button: Starts full pipeline

Main Content Area:

When no data loaded:
â”œâ”€ Information message: "Upload a CSV or generate a dataset..."
â””â”€ Application halts until data available

After clicking "Train & Compare":

1. COMPARATIVE RESULTS TABLE
   â”œâ”€ Model column: Model name
   â”œâ”€ Performance metrics:
   â”‚  â”œâ”€ Regression: R2, MSE, RMSE
   â”‚  â””â”€ Classification: Accuracy, F1, Precision, Recall
   â”œâ”€ Training & inference time (seconds)
   â”œâ”€ CO2 emissions (kg)
   â”œâ”€ Cross-validation mean score (if enabled)
   â””â”€ Sorted by primary metric (descending)

2. SUCCESS MESSAGE
   â”œâ”€ Text: "Top performer by [METRIC]: [MODEL_NAME]"
   â””â”€ Color: Green checkmark icon

3. DIAGNOSTICS SECTION
   â”œâ”€ Regression Mode:
   â”‚  â”œâ”€ Left (50%): Residual Distribution Histogram
   â”‚  â”‚  â””â”€ Shows: Distribution of (y_true - y_pred)
   â”‚  â””â”€ Right (50%): True vs Predicted Scatter Plot
   â”‚     â”œâ”€ Sample: 10,000 points
   â”‚     â””â”€ Shows: Prediction accuracy visually
   â”‚
   â””â”€ Classification Mode:
      â”œâ”€ Left (50%): Confusion Matrix Heatmap
      â”‚  â”œâ”€ Shows: True labels vs predictions
      â”‚  â””â”€ Color intensity: Frequency
      â””â”€ Right (50%): Class Distribution Bar Chart
         â””â”€ Shows: Count per class in test set

4. SCENARIO TESTER SECTION
   â”œâ”€ 3-column input layout
   â”‚  â”œâ”€ Column 1:
   â”‚  â”‚  â”œâ”€ RelativeCompactness slider
   â”‚  â”‚  â”œâ”€ Orientation dropdown
   â”‚  â”‚  â””â”€ BuildingType dropdown
   â”‚  â”‚
   â”‚  â”œâ”€ Column 2:
   â”‚  â”‚  â”œâ”€ SurfaceArea slider
   â”‚  â”‚  â”œâ”€ GlazingAreaDistribution dropdown
   â”‚  â”‚  â””â”€ WallArea slider
   â”‚  â”‚
   â”‚  â””â”€ Column 3:
   â”‚     â”œâ”€ RoofArea slider
   â”‚     â””â”€ OverallHeight slider
   â”‚
   â”œâ”€ Prediction Results Table:
   â”‚  â”œâ”€ Model name column
   â”‚  â”œâ”€ Prediction column (predicted value)
   â”‚  â”œâ”€ Optional: Prediction class (classification)
   â”‚  â””â”€ Display all selected models' predictions
   â”‚
   â””â”€ Features: Real-time update on parameter change


PERFORMANCE CHARACTERISTICS
================================================================================

Training Time Estimates (Single Model on 500k rows):

Model                    | CPU Time (s) | Memory (GB) | Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LinearRegression         | 0.5-1        | 0.2         | Baseline reference
Ridge                    | 0.5-1        | 0.2         | Similar to Linear
Lasso                    | 2-5          | 0.2         | Iterative optimization
LogisticRegression       | 5-10         | 0.3         | Multi-class increases time
RandomForestRegressor    | 10-20        | 2-3         | 200 trees, parallelized
RandomForestClassifier   | 15-25        | 2.5-3.5     | 300 trees, parallelized
GradientBoostingReg      | 15-30        | 1-2         | Sequential tree growth
GradientBoostingCls      | 20-40        | 1-2         | Slower for multi-class
XGBRegressor             | 5-15         | 1-2         | Histogram method faster
XGBClassifier            | 8-20         | 1-2         | Multi-class loss slower
LGBMRegressor            | 3-10         | 0.5-1       | Fastest tree method
LGBMClassifier           | 4-12         | 0.5-1       | Leaf-wise growth efficient
CatBoostRegressor        | 8-15         | 1-1.5       | Categorical handling
CatBoostClassifier       | 10-20        | 1-1.5       | Bootstrap optimization

Inference Time per Sample:
â€¢ Linear models: <1 ms (microseconds actually)
â€¢ Tree-based models: 0.1-1 ms per sample
â€¢ Ensembles scale with number of trees

COâ‚‚ Emissions Estimates (Typical):
â€¢ Small model (Linear): 1e-7 to 1e-6 kg COâ‚‚
â€¢ Medium model (Random Forest): 1e-6 to 1e-5 kg COâ‚‚
â€¢ Large model (XGBoost 500k rows): 1e-5 to 1e-4 kg COâ‚‚


CODE QUALITY & ERROR HANDLING
================================================================================

Safe Library Imports:
â€¢ Optional models (XGBoost, LightGBM, CatBoost) fail gracefully
â€¢ safe_import() function: Returns None if library missing
â€¢ No application crash if optional dependency unavailable
â€¢ User selects from available models only

Data Validation:
â€¢ NaN/Inf detection and removal: df.replace([np.inf, -np.inf], np.nan)
â€¢ Duplicate elimination: df.drop_duplicates()
â€¢ Feature-target alignment: Automatic removal of target from features
â€¢ Categorical encoding robustness: drop_first=True prevents multicollinearity

Error Recovery:
â€¢ Emissions tracker: try/except wrapper allows operation without CodeCarbon
â€¢ Model training: Graceful degradation if model unavailable
â€¢ Scenario prediction: Column alignment before prediction
â€¢ Scaler application: Conditional checks for scaler existence

State Management:
â€¢ Streamlit @st.cache_data: Caches synthetic data generation
  â””â”€ Prevents regeneration on reruns (significant speedup)
â€¢ Session state: Implicit caching of uploaded files
â€¢ Widget values: Persist across application reruns


INSTALLATION & SETUP
================================================================================

Prerequisites:
â€¢ Python 3.10+ (tested with 3.12.1)
â€¢ pip package manager
â€¢ Virtual environment (recommended)

Step 1: Clone Repository
$ git clone https://github.com/RoshanNaidu/Hack-Earth.git
$ cd Hack-Earth

Step 2: Create Virtual Environment (Recommended)
$ python3 -m venv venv
$ source venv/bin/activate  # On Windows: venv\Scripts\activate

Step 3: Install Required Dependencies
$ pip install -r requirements.txt

Or manually:
$ pip install numpy pandas scikit-learn streamlit matplotlib

Step 4: Install Optional Dependencies (Recommended)
$ pip install xgboost lightgbm catboost codecarbon

Step 5: Run Application
$ streamlit run app.py

Step 6: Access Web Interface
Default: http://localhost:8501
Browser should open automatically; if not, visit the URL above.


USAGE EXAMPLES
================================================================================

EXAMPLE 1: Quick Baseline Model Comparison
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Start application: streamlit run app.py
2. In Sidebar:
   â”œâ”€ Data: Click "Generate synthetic (500k rows)"
   â”œâ”€ Task: Select "Regression"
   â”œâ”€ Target: HeatingLoad (default)
   â”œâ”€ Test Size: 20%
   â”œâ”€ Max rows: 200,000 (faster)
   â”œâ”€ Models: Select "LinearRegression", "Ridge", "RandomForestRegressor"
   â”œâ”€ Scale features: â˜‘ (checked)
   â”œâ”€ CV: â˜ (unchecked - speed priority)
   â””â”€ Click: ğŸš€ Train & Compare

Expected Results:
â€¢ LinearRegression RÂ²: ~0.85
â€¢ Ridge RÂ²: ~0.85
â€¢ RandomForestRegressor RÂ²: ~0.95
â€¢ Total time: ~30 seconds
â€¢ COâ‚‚: ~0.0001 kg


EXAMPLE 2: Comprehensive Model Evaluation with Cross-Validation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Start application
2. In Sidebar:
   â”œâ”€ Data: Upload your custom energy_data.csv
   â”œâ”€ Task: Select "Classification"
   â”œâ”€ Base column: HeatingLoad
   â”œâ”€ Bins: 4 (create 4 efficiency classes)
   â”œâ”€ Binning: Quantile (balanced classes)
   â”œâ”€ Test Size: 15%
   â”œâ”€ Max rows: 500,000 (use all data)
   â”œâ”€ Models: Select all available
   â”œâ”€ Scale: â˜‘
   â”œâ”€ CV: â˜‘ (enable 5-fold)
   â”œâ”€ Emissions: â˜‘
   â””â”€ Click: ğŸš€ Train & Compare

Results Section:
â€¢ Detailed comparison of 6+ classifiers
â€¢ CV scores show generalization performance
â€¢ Confusion matrix reveals misclassification patterns
â€¢ Emissions tracking shows computational cost

Time estimate: 2-5 minutes (depending on dataset size)


EXAMPLE 3: What-If Building Optimization Analysis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Run initial training (any mode)
2. In Scenario Tester section, adjust parameters:

   Building A: Current State
   â”œâ”€ RelativeCompactness: 0.75
   â”œâ”€ SurfaceArea: 250 mÂ²
   â”œâ”€ WallArea: 150 mÂ²
   â”œâ”€ RoofArea: 100 mÂ²
   â”œâ”€ Height: 5 m
   â”œâ”€ Orientation: South
   â”œâ”€ Glazing: Uniform
   â””â”€ Type: Commercial

   Building B: Optimized (Reduce heating load)
   â”œâ”€ RelativeCompactness: 0.95 (â†‘ more compact)
   â”œâ”€ SurfaceArea: 200 mÂ² (â†“ smaller)
   â”œâ”€ WallArea: 100 mÂ² (â†“ less surface)
   â”œâ”€ RoofArea: 80 mÂ² (â†“ smaller)
   â”œâ”€ Height: 4 m (â†“ shorter)
   â”œâ”€ Orientation: North (â†“ less solar gain)
   â”œâ”€ Glazing: North-heavy (â†“ minimized)
   â””â”€ Type: Commercial

Interpretation:
â€¢ Best model predictions for scenario A vs B
â€¢ Quantify efficiency improvements
â€¢ Guide architectural design decisions


TROUBLESHOOTING
================================================================================

Issue: "No module named 'xgboost'"
Solution: Models gracefully skip; install optional: pip install xgboost

Issue: "Code Carbon tracker error"
Solution: App continues without emissions tracking; install: pip install codecarbon

Issue: "Memory error on 500k rows"
Solution: Reduce max_rows in sidebar (default 200k) or use smaller dataset

Issue: "Train time very long"
Solution: 
  â”œâ”€ Reduce max_rows
  â”œâ”€ Disable cross-validation (uncheck CV checkbox)
  â””â”€ Select fewer models

Issue: "Poor model performance"
Diagnosis:
  â”œâ”€ Check residual plot for systematic bias
  â”œâ”€ Review confusion matrix for specific misclassifications
  â”œâ”€ Consider feature engineering
  â”œâ”€ Try different binning strategy (quantile vs uniform)
  â””â”€ Increase random_state for reproducibility

Issue: "Predictions seem unrealistic"
Solution:
  â”œâ”€ Verify input ranges match training data
  â”œâ”€ Check that features scaled correctly
  â”œâ”€ Review scenario inputs in Scenario Tester
  â””â”€ Compare against best-performing model


FILE SPECIFICATIONS
================================================================================

app.py (Main Application)
â”œâ”€ Size: 516 lines
â”œâ”€ Language: Python 3.10+
â”œâ”€ Key Sections:
â”‚  â”œâ”€ Imports (lines 1-45)
â”‚  â”œâ”€ Helper functions (lines 46-200)
â”‚  â”œâ”€ Sidebar UI (lines 200-320)
â”‚  â”œâ”€ Data preprocessing (lines 320-410)
â”‚  â”œâ”€ Model training (lines 410-500)
â”‚  â”œâ”€ Results visualization (lines 500-516)
â”‚  â””â”€ Scenario testing (embedded in visualization)
â”œâ”€ Functions:
â”‚  â”œâ”€ safe_import() - Safely load optional libraries
â”‚  â”œâ”€ get_emissions_tracker() - Initialize CodeCarbon
â”‚  â”œâ”€ generate_synthetic_500k() - Create synthetic dataset
â”‚  â”œâ”€ build_model_zoo() - Instantiate all models
â”‚  â”œâ”€ evaluate_model() - Train and evaluate single model
â”‚  â””â”€ range_for() - Get slider ranges from data
â”œâ”€ Decorators:
â”‚  â””â”€ @st.cache_data - Cache expensive operations
â””â”€ Dependencies: 28 external libraries

emissions.csv (Emissions Log)
â”œâ”€ Format: CSV (comma-separated)
â”œâ”€ Rows: Variable (appended after each training run)
â”œâ”€ Columns: 42 fields
â”œâ”€ Key Columns:
â”‚  â”œâ”€ timestamp: ISO 8601 format (UTC)
â”‚  â”œâ”€ project_name: "EnergyEfficiencyApp"
â”‚  â”œâ”€ run_id: Unique ID (UUID4)
â”‚  â”œâ”€ duration: Training time in seconds
â”‚  â”œâ”€ emissions: COâ‚‚ in kg
â”‚  â”œâ”€ cpu_power: CPU power draw in watts
â”‚  â”œâ”€ gpu_power: GPU power draw in watts
â”‚  â”œâ”€ ram_power: RAM power draw in watts
â”‚  â”œâ”€ energy_consumed: Total energy in kWh
â”‚  â”œâ”€ country_name: Location name
â”‚  â”œâ”€ cpu_model: Processor model string
â”‚  â”œâ”€ python_version: Python version string
â”‚  â””â”€ [36 more fields...]
â”œâ”€ Example Entry:
â”‚  timestamp: 2026-02-02T06:08:26
â”‚  emissions: 2.2414e-06 kg
â”‚  duration: 1.0757 seconds
â””â”€ Purpose: Track environmental impact, audit model training

*.bak files (Backup Copies)
â”œâ”€ Format: CSV backup archives
â”œâ”€ Purpose: Recovery and version history
â”œâ”€ Note: Can be deleted safely (backups only)


LICENSE INFORMATION
================================================================================

License Type: Apache License 2.0
Full Text: See LICENSE file

Key Points:
â€¢ Open-source software
â€¢ Permissive free use
â€¢ Include license in distributions
â€¢ No warranty provided
â€¢ User assumes all responsibility
â€¢ Attribution appreciated but not required
â€¢ Can modify and redistribute


CONTRIBUTING & DEVELOPMENT
================================================================================

Repository: github.com/RoshanNaidu/Hack-Earth
Current Branch: main
Default Branch: main

Future Enhancement Ideas:
â€¢ Deploying on Deep Learning Models (Neural Networks)
â€¢ Model explanation features (SHAP values)
â€¢ Hyperparameter optimization (Optuna/Hyperopt)
â€¢ Ensemble stacking/voting mechanisms
â€¢ Feature importance visualizations
â€¢ Time-series forecasting (if temporal data)
â€¢ Database integration (SQLite/PostgreSQL)
â€¢ Multi-GPU acceleration
â€¢ Mobile-friendly responsive design
â€¢ Model persistence (save/load trained models)
â€¢ API endpoint exposure
â€¢ Real-time deployment capabilities
â€¢ Batch prediction interface


DEPLOYMENT CONSIDERATIONS
================================================================================

Streamlit Sharing:
$ streamlit share run app.py
  â”œâ”€ Host for free on Streamlit Cloud
  â”œâ”€ Requires GitHub connection
  â””â”€ Auto-deployed from repo

Docker Deployment:
$ docker build -t hack-earth .
$ docker run -p 8501:8501 hack-earth
  â”œâ”€ Create Dockerfile (example below)
  â””â”€ Container isolation + reproducibility

Dockerfile Template:
FROM python:3.12-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY app.py .
EXPOSE 8501
CMD ["streamlit", "run", "app.py"]

Environment Variables:
â€¢ STREAMLIT_SERVER_PORT: 8501 (default)
â€¢ STREAMLIT_SERVER_ADDRESS: localhost (default)
â€¢ PYTHONUNBUFFERED: 1 (logs without buffering)


PERFORMANCE OPTIMIZATION TIPS
================================================================================

For Faster Training:

1. SUBSAMPLE DATA
   â””â”€ Set max_rows to 50,000-100,000 for rapid experiments
   â””â”€ Full 500k only needed for final evaluation

2. DISABLE FEATURES
   â”œâ”€ Turn off cross-validation for speed
   â”œâ”€ Disable emissions tracking
   â””â”€ Select fewer models to compare

3. FEATURE SELECTION
   â”œâ”€ Standardize numeric features (faster convergence)
   â”œâ”€ One-hot encoding is done automatically
   â””â”€ Consider dropping low-variance features

4. PARALLELIZATION
   â””â”€ Already enabled: n_jobs=-1 on RandomForest/GradientBoosting
   â””â”€ Uses all CPU cores available

5. CACHING
   â””â”€ Synthetic data generation cached automatically
   â””â”€ Subsequent reruns load instantly

6. HARDWARE
   â”œâ”€ Use GPU: XGBoost tree_method="gpu_hist" (if NVIDIA GPU)
   â”œâ”€ More RAM: Reduce feature dimensionality
   â””â”€ More cores: Parallel models benefit


CONCLUSION & QUICK START
================================================================================

Quick Start (30 seconds):

1. Clone/download: Hack-Earth repository
2. Terminal: cd Hack-Earth
3. Terminal: pip install -r requirements.txt
4. Terminal: streamlit run app.py
5. Browser: Opens http://localhost:8501 automatically
6. Sidebar: Generate synthetic dataset (500k rows)
7. Sidebar: Select 3-4 models
8. Click: ğŸš€ Train & Compare
9. View: Results, diagnostics, scenarios

Key Takeaways:
âœ“ End-to-end ML platform in single Python file (516 lines)
âœ“ Supports 8+ regression models, 6+ classification algorithms
âœ“ Automatically tracks computational carbon emissions
âœ“ Interactive "what-if" scenario analysis interface
âœ“ Comprehensive model evaluation and visualization
âœ“ Graceful handling of optional dependencies
âœ“ Production-ready code with error handling
âœ“ Scalable to 500,000+ rows with subsampling support

For detailed analysis: Review app.py source code with inline comments

Questions or Issues: github.com/RoshanNaidu/Hack-Earth/issues

================================================================================
                              END OF README
================================================================================
Version: 1.0
Last Updated: February 2, 2026
Document Scope: Comprehensive end-to-end project documentation
================================================================================
